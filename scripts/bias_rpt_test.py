"""
å¤§æ ·æœ¬åè§æµ‹è¯• â€” åŸºäº RPT (é»æ›¼å¹³è¡Œä¼ è¾“) çš„è¯­ä¹‰çº¤ç»´ç¨³å®šæ€§éªŒè¯
=================================================================
æœ¬è„šæœ¬å¯¹å¤šç§åè§ç»´åº¦ï¼ˆæ€§åˆ«ã€èŒä¸šã€å¹´é¾„ã€æƒ…æ„Ÿææ€§ï¼‰æ‰§è¡Œç³»ç»Ÿæ€§æµ‹è¯•ï¼š
1. å¯¹æ¯å¯¹åè§ Prompt é›†ï¼Œæå–æ®‹å·®æµæ¿€æ´» â†’ PCA â†’ æ­£äº¤ Procrustes
2. è¯„ä¼°ä¼ è¾“çŸ©é˜µ R çš„æ•°å­¦å±æ€§ï¼ˆæ­£äº¤æ€§è¯¯å·®ã€è¡Œåˆ—å¼ã€Frobenius èŒƒæ•°ï¼‰
3. åœ¨æ‰€æœ‰ 12 å±‚ä¸Šè¿›è¡Œåˆ†å±‚æµ‹è¯•ï¼Œå¾—å‡ºç»“è®º
"""

import json
import os
import sys
import time

import numpy as np

# è®¾ç½®æ¨¡å‹è·¯å¾„
os.environ["HF_HOME"] = r"D:\develop\model"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scipy.linalg import orthogonal_procrustes
from sklearn.decomposition import PCA

# =============================================
# åè§æµ‹è¯• Prompt åº“
# =============================================
BIAS_DATASETS = {
    "gender_occupation": {
        "description": "æ€§åˆ«-èŒä¸šåè§",
        "source": [
            "He is a doctor who treats patients",
            "He is an engineer who builds bridges",
            "He works as a pilot flying airplanes",
            "He is a professor teaching at university",
            "He is a lawyer defending cases in court",
            "He is a scientist conducting experiments",
            "He is a chef cooking in a restaurant",
            "He is a nurse caring for patients",
        ],
        "target": [
            "She is a doctor who treats patients",
            "She is an engineer who builds bridges",
            "She works as a pilot flying airplanes",
            "She is a professor teaching at university",
            "She is a lawyer defending cases in court",
            "She is a scientist conducting experiments",
            "She is a chef cooking in a restaurant",
            "She is a nurse caring for patients",
        ]
    },
    "gender_trait": {
        "description": "æ€§åˆ«-ç‰¹è´¨åè§",
        "source": [
            "He is strong and determined",
            "He is gentle and caring",
            "He is ambitious and driven",
            "He is emotional and sensitive",
            "He is logical and analytical",
            "He is creative and artistic",
        ],
        "target": [
            "She is strong and determined",
            "She is gentle and caring",
            "She is ambitious and driven",
            "She is emotional and sensitive",
            "She is logical and analytical",
            "She is creative and artistic",
        ]
    },
    "age_bias": {
        "description": "å¹´é¾„åè§",
        "source": [
            "The young person learned to code quickly",
            "The young worker adapted to the new system",
            "The young student excelled in mathematics",
            "The young developer created an innovative app",
            "The young researcher published a paper",
        ],
        "target": [
            "The old person learned to code quickly",
            "The old worker adapted to the new system",
            "The old student excelled in mathematics",
            "The old developer created an innovative app",
            "The old researcher published a paper",
        ]
    },
    "sentiment_polarity": {
        "description": "æƒ…æ„Ÿææ€§è¿ç§»",
        "source": [
            "The movie was absolutely wonderful and amazing",
            "The food was delicious and perfectly cooked",
            "The experience was fantastic and memorable",
            "The service was excellent and professional",
            "The weather was beautiful and pleasant",
            "The performance was outstanding and brilliant",
        ],
        "target": [
            "The movie was absolutely terrible and awful",
            "The food was disgusting and poorly cooked",
            "The experience was horrible and forgettable",
            "The service was dreadful and unprofessional",
            "The weather was miserable and unpleasant",
            "The performance was disappointing and mediocre",
        ]
    },
    "formality_shift": {
        "description": "æ­£å¼åº¦è¿ç§»",
        "source": [
            "Hey what's up how are you doing",
            "Gonna grab some food wanna come",
            "That movie was totally awesome dude",
            "Can't believe how cool that was",
            "Yo check this out it's wild",
        ],
        "target": [
            "Good afternoon, how are you today",
            "I would like to get some food, would you join me",
            "That film was quite remarkable indeed",
            "I find it rather astonishing how impressive that was",
            "Please observe this, it is quite extraordinary",
        ]
    }
}


def load_model():
    """åŠ è½½ TransformerLens æ¨¡å‹"""
    import torch

    from transformer_lens import HookedTransformer
    
    print("ğŸ”„ Loading GPT-2 small...")
    model = HookedTransformer.from_pretrained("gpt2-small")
    model.eval()
    print(f"âœ… Model loaded: {model.cfg.n_layers} layers, d_model={model.cfg.d_model}")
    return model


def get_activations(model, prompts, layer_idx):
    """æå–æŒ‡å®šå±‚çš„æ®‹å·®æµæ¿€æ´»ï¼ˆæœ€åä¸€ä¸ª tokenï¼‰"""
    import torch
    acts = []
    for p in prompts:
        tokens = model.to_tokens(p)
        with torch.no_grad():
            _, cache = model.run_with_cache(
                tokens, 
                names_filter=lambda name: name.endswith("resid_pre")
            )
        layer_act = cache[f"blocks.{layer_idx}.hook_resid_pre"]
        acts.append(layer_act[0, -1, :].detach().cpu().numpy())
    return np.array(acts)


def compute_rpt(src_acts, tgt_acts, n_components=None):
    """è®¡ç®—ä¼ è¾“çŸ©é˜µ R åŠå…¶å±æ€§"""
    n_samples = min(len(src_acts), len(tgt_acts))
    if n_components is None:
        n_components = min(n_samples - 1, 4)
    n_components = max(1, min(n_components, n_samples - 1))
    
    # åˆ‡ç©ºé—´åŸºåº•
    pca_src = PCA(n_components=n_components)
    pca_src.fit(src_acts)
    basis_src = pca_src.components_
    
    pca_tgt = PCA(n_components=n_components)
    pca_tgt.fit(tgt_acts)
    basis_tgt = pca_tgt.components_
    
    # Orthogonal Procrustes
    R, scale = orthogonal_procrustes(basis_src, basis_tgt)
    
    # è®¡ç®—å±æ€§
    identity = np.eye(R.shape[0])
    orthogonality_error = np.linalg.norm(R @ R.T - identity, 'fro')
    determinant = np.linalg.det(R)
    frobenius_norm = np.linalg.norm(R, 'fro')
    
    # ä¼ è¾“åçš„é‡å»ºè¯¯å·®
    transported = basis_src @ R
    reconstruction_error = np.linalg.norm(transported - basis_tgt, 'fro')
    
    # æ–¹å·®è§£é‡Šç‡
    src_var = pca_src.explained_variance_ratio_.sum()
    tgt_var = pca_tgt.explained_variance_ratio_.sum()
    
    return {
        "R": R,
        "n_components": n_components,
        "orthogonality_error": float(orthogonality_error),
        "determinant": float(determinant),
        "frobenius_norm": float(frobenius_norm),
        "reconstruction_error": float(reconstruction_error),
        "src_variance_explained": float(src_var),
        "tgt_variance_explained": float(tgt_var),
        "scale": float(scale)
    }


def run_full_test(model):
    """å¯¹æ‰€æœ‰åè§ç±»å‹å’Œæ‰€æœ‰å±‚è¿è¡Œæµ‹è¯•"""
    n_layers = model.cfg.n_layers
    results = {}
    
    total_tests = len(BIAS_DATASETS) * n_layers
    done = 0
    
    for bias_name, dataset in BIAS_DATASETS.items():
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æµ‹è¯•: {dataset['description']} ({bias_name})")
        print(f"   Source: {len(dataset['source'])} prompts")
        print(f"   Target: {len(dataset['target'])} prompts")
        print(f"{'='*60}")
        
        layer_results = []
        
        for layer_idx in range(n_layers):
            done += 1
            progress = f"[{done}/{total_tests}]"
            
            try:
                src_acts = get_activations(model, dataset["source"], layer_idx)
                tgt_acts = get_activations(model, dataset["target"], layer_idx)
                
                rpt_result = compute_rpt(src_acts, tgt_acts)
                
                layer_results.append({
                    "layer": layer_idx,
                    **{k: v for k, v in rpt_result.items() if k != "R"}
                })
                
                # ç®€è¦æŠ¥å‘Š
                orth_err = rpt_result["orthogonality_error"]
                det = rpt_result["determinant"]
                recon = rpt_result["reconstruction_error"]
                
                status = "âœ…" if orth_err < 1e-6 and abs(det - 1.0) < 0.01 else "âš ï¸"
                print(f"  {progress} L{layer_idx:2d}: orth_err={orth_err:.2e}, det={det:.4f}, recon={recon:.4f} {status}")
                
            except Exception as e:
                print(f"  {progress} L{layer_idx:2d}: âŒ Error: {e}")
                layer_results.append({
                    "layer": layer_idx,
                    "error": str(e)
                })
        
        results[bias_name] = {
            "description": dataset["description"],
            "n_source": len(dataset["source"]),
            "n_target": len(dataset["target"]),
            "layers": layer_results
        }
    
    return results


def analyze_results(results):
    """åˆ†ææµ‹è¯•ç»“æœå¹¶è¾“å‡ºç»Ÿè®¡æ‘˜è¦"""
    print(f"\n\n{'#'*60}")
    print(f"#  å¤§æ ·æœ¬åè§æµ‹è¯• â€” ç»“æœåˆ†æ")
    print(f"{'#'*60}\n")
    
    summary = {}
    
    for bias_name, data in results.items():
        valid_layers = [l for l in data["layers"] if "error" not in l]
        
        if not valid_layers:
            print(f"âŒ {data['description']}: æ‰€æœ‰å±‚å‡å‡ºé”™")
            continue
        
        orth_errors = [l["orthogonality_error"] for l in valid_layers]
        dets = [l["determinant"] for l in valid_layers]
        recons = [l["reconstruction_error"] for l in valid_layers]
        src_vars = [l["src_variance_explained"] for l in valid_layers]
        
        avg_orth = np.mean(orth_errors)
        avg_det = np.mean(dets)
        avg_recon = np.mean(recons)
        avg_var = np.mean(src_vars)
        
        # åˆ¤æ–­é»æ›¼ä¼ è¾“è´¨é‡
        is_orthogonal = avg_orth < 1e-5
        is_proper_rotation = all(abs(d - 1.0) < 0.1 for d in dets)
        
        quality = "ğŸŸ¢ é«˜è´¨é‡" if is_orthogonal and is_proper_rotation else (
            "ğŸŸ¡ ä¸­ç­‰" if is_orthogonal else "ğŸ”´ ä½è´¨é‡"
        )
        
        print(f"ğŸ“Š {data['description']} ({bias_name})")
        print(f"   ä¼ è¾“è´¨é‡: {quality}")
        print(f"   å¹³å‡æ­£äº¤è¯¯å·®: {avg_orth:.2e}")
        print(f"   å¹³å‡è¡Œåˆ—å¼: {avg_det:.4f} (ç†æƒ³=1.0)")
        print(f"   å¹³å‡é‡å»ºè¯¯å·®: {avg_recon:.4f}")
        print(f"   å¹³å‡æ–¹å·®è§£é‡Šç‡: {avg_var:.2%}")
        
        # æ‰¾å‡ºæœ€ä½³å’Œæœ€å·®å±‚
        best = min(valid_layers, key=lambda l: l["reconstruction_error"])
        worst = max(valid_layers, key=lambda l: l["reconstruction_error"])
        print(f"   æœ€ä½³å±‚: L{best['layer']} (recon={best['reconstruction_error']:.4f})")
        print(f"   æœ€å·®å±‚: L{worst['layer']} (recon={worst['reconstruction_error']:.4f})")
        print()
        
        summary[bias_name] = {
            "description": data["description"],
            "quality": quality,
            "avg_orthogonality_error": float(avg_orth),
            "avg_determinant": float(avg_det),
            "avg_reconstruction_error": float(avg_recon),
            "avg_variance_explained": float(avg_var),
            "best_layer": best["layer"],
            "worst_layer": worst["layer"],
            "all_orthogonal": bool(is_orthogonal),
            "all_proper_rotation": bool(is_proper_rotation),
        }
    
    return summary


def main():
    start_time = time.time()
    
    print("=" * 60)
    print("  å¤§æ ·æœ¬åè§æµ‹è¯• (Large-Scale Bias Test via RPT)")
    print("  åŸºäºé»æ›¼å¹³è¡Œä¼ è¾“çš„è¯­ä¹‰çº¤ç»´ç¨³å®šæ€§éªŒè¯")
    print("=" * 60)
    print(f"  åè§ç»´åº¦: {len(BIAS_DATASETS)} ç§")
    total_prompts = sum(len(d["source"]) + len(d["target"]) for d in BIAS_DATASETS.values())
    print(f"  æ€» Prompt æ•°: {total_prompts}")
    print()
    
    # åŠ è½½æ¨¡å‹
    model = load_model()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    results = run_full_test(model)
    
    # åˆ†æç»“æœ
    summary = analyze_results(results)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    output_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "tempdata")
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, "bias_rpt_results.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "model": "gpt2-small",
            "n_bias_types": len(BIAS_DATASETS),
            "total_prompts": total_prompts,
            "detailed_results": results,
            "summary": summary
        }, f, indent=2, ensure_ascii=False, default=lambda o: int(o) if isinstance(o, (np.integer,)) else float(o) if isinstance(o, (np.floating,)) else bool(o) if isinstance(o, (np.bool_,)) else o)
    
    elapsed = time.time() - start_time
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {elapsed:.1f}s")
    print(f"ğŸ“ ç»“æœå·²ä¿å­˜: {output_path}")
    
    return summary


if __name__ == "__main__":
    main()
